[
["introduction.html", "Smart Data Project Part 1 Introduction 1.1 Initial problem 1.2 Outline", " Smart Data Project Hugo Brehier, Florentin Coeurdoux 2020-02-13 Part 1 Introduction 1.1 Initial problem Our task involves the training of a classifier specialized in recognizing airplanes constructor and model type. The idea is to put into practice our courses of Deep Learning, especially Convolution Neural Networks (CNN). Indeed, these models have gained a lot of traction in the last decade or so, in computer vision, after setting the best accuracy in classification contests, such as AlexNet on ImageNet. We will first look into classyfing Airbus and Boeing airplanes. Afterwards, we will delve into model types. In modern fashion, the data used to train our models is scrapped from the website planespotters.net. To manage the planning and monitoring of the various tasks throughout the project we organized ourselves thanks to a project management website : Trello. Our trello dashboard 1.2 Outline In latter parts, we will describe the following points: the available data the methods and tools used the results obtained The structure of our report will be as follow: Website scrapping Exploratory analysis Classification through Deep Learning Deploying to production "],
["web-scrapping.html", "Part 2 Web Scrapping 2.1 What is Web Scraping? 2.2 The available data 2.3 Website extraction", " Part 2 Web Scrapping 2.1 What is Web Scraping? Web scraping is a technique for converting the data present in unstructured format (HTML tags) over the web to the structured format which can easily be accessed and used. Almost all the main languages provide ways for performing web scraping. To remain consistent in the same programming language we have chosen to use the R language for scraping the data from the planespotters.com website. 2.2 The available data This website contains thousands of aircrafts pictures as well as the information about the constructor, the model and the airline for each different aircraft. Our aim is to extract all these informations. 2.3 Website extraction Fisrt we have to load the different libraries. library(rvest) library(dplyr) library(stringr) The R package rvest (Wickham (2019)) is a package that makes it easy to scrape data from html web pages, inspired by python libraries like beautiful soup. It is designed to work with magrittr so that you can express complex operations as pipelines. The html_attr function allows to extract attributes, text and tag name from html. The website contains 125 pages and each pages contains 48 differents pictures of airplanes with descriptions. Each images Here is our entire website images extraction : aicrafts &lt;- matrix(ncol = 3, nrow = 48*125) nb &lt;- 0 for (i in 1:125){ nb &lt;- nb+1 planespotter_url &lt;- paste(&quot;https://www.planespotters.net/photos/latest?page=&quot;, i, sep = &quot;&quot;) planespotter &lt;- read_html(planespotter_url) img &lt;- planespotter %&gt;% html_node(xpath = &#39;//*/img&#39;) imgalt &lt;- html_attr(img, &#39;alt&#39;) imgsrc &lt;- html_attr(img, &#39;src&#39;) imgsrc_split &lt;- strsplit(imgsrc, &quot;_&quot;) img_number &lt;- as.integer(imgsrc_split[[1]][3]) constructor_html &lt;- planespotter %&gt;% html_nodes(paste0(&#39;#&#39;, img_number, &quot; a&quot;)) %&gt;% html_attr(&#39;href&#39;) constructor &lt;- stringr::str_replace(constructor_html[4], &quot;/photos/manufacturer/&quot;, &quot;&quot;) aircraft_type &lt;- stringr::str_replace(constructor_html[5], paste0(&#39;/photos/aircraft/&#39;,constructor, &#39;/&#39; ), &quot;&quot;) airline &lt;- stringr::str_replace(constructor_html[3], &quot;/airline/&quot;, &quot;&quot;) download.file(imgsrc, destfile = paste0(&#39;Images/&#39;,constructor, &#39;.&#39;, nb, &#39;.jpg&#39;)) aicrafts[(nb-co), 1] &lt;- constructor aicrafts[(nb-co), 2] &lt;- aircraft_type aicrafts[(nb-co), 3] &lt;- airline for(j in 1:47) { nb &lt;- nb+1 img_numbera &lt;- img_number - j node_id &lt;- paste0(&#39;#&#39;, img_numbera, &#39; .photo__intrinsic_item&#39;) img &lt;- planespotter %&gt;% html_node(node_id) constructor_html &lt;- planespotter %&gt;% html_nodes(paste0(&#39;#&#39;, img_numbera, &quot; a&quot;)) %&gt;% html_attr(&#39;href&#39;) constructor &lt;- stringr::str_replace(constructor_html[4], &quot;/photos/manufacturer/&quot;, &quot;&quot;) aircraft_type &lt;- stringr::str_replace(constructor_html[5], paste0(&#39;/photos/aircraft/&#39;,constructor, &#39;/&#39;),&quot;&quot;) airline &lt;- stringr::str_replace(constructor_html[3], &quot;/airline/&quot;, &quot;&quot;) imgalt &lt;- html_attr(img, &#39;alt&#39;) imgsrc &lt;- html_attr(img, &#39;src&#39;) aicrafts[(nb-co), 1] &lt;- constructor aicrafts[(nb-co), 2] &lt;- aircraft_type aicrafts[(nb-co), 3] &lt;- airline if(is.na(imgsrc)){ Sys.sleep(0.5) }else{ download.file(imgsrc, destfile = paste0(&#39;Images/&#39;,constructor, &#39;.&#39;, nb, &#39;.jpg&#39;)) } } } aircrafts &lt;- data_frame(constructor = aicrafts[,1], aircraft_type = aicrafts[,2], airline = aicrafts[,3]) write.table(aircrafts, file = &quot;aicrafts.csv&quot;, append = T) View(aircrafts) length(list.files(&quot;Images/&quot;, pattern=&quot;jpg&quot;)) dim(aicrafts) This code extract all images of the website in addition with creating a dataframe with characteristics of the planes. References "],
["exploratory-data-analysis.html", "Part 3 Exploratory Data Analysis 3.1 Load the data 3.2 Top constructor 3.3 Top airlines 3.4 Proportion of model for Airbus 3.5 Proportion of model for Boeing", " Part 3 Exploratory Data Analysis 3.1 Load the data The count of NA is : ## [1] 87 Without NA, our data has the following size: ## [1] 2553 4 Unfortunately as you can see there is a lots of NAs in the scrapped data. 3.2 Top constructor Here we directly anderstand that we won’t be able to train a CNN for other constructor than Airbus and Boeing. 3.3 Top airlines Here, unlike for the constructors the distribution of airlines is more homogeneous, to verify this claim we can plot all of the airlines. So we may be able to train a cnn to recognize the airline of a plane, at least for the most frequent one. Here we observe there is also a disparity in airlines frequency but much less present than for the constructors. 3.4 Proportion of model for Airbus There exist 20 different models of Airbus in our data. The most frequent Airbus is by far the A320-200. 3.5 Proportion of model for Boeing There exist 30 different models of Boeing in our data. The most frequent Airbus is by far the A320-200. "],
["deep-learning.html", "Part 4 Deep Learning 4.1 Overview 4.2 Model 4.3 Performance check", " Part 4 Deep Learning 4.1 Overview The fundamental idea which led to the development of CNN, is the idea that detecting an item in an image should trigger a response (neuron activation) without care for the position in the image of the item. Convolutions ,with filters invariant by translation across the image, follow this idea (Mallat (2016)). By using several convolutions in parallel and in succession, a network is able to learn some features of the data. Classification is finally achieved by vectorizing the last layer which feeds a classical multilayer perceptron (MLP). The first groundbreaking model is AlexNet, developed at the University of Toronto (Krizhevsky, Sutskever, and Hinton (2012)). It achieved first place at ImageNet image classification competition in 2012. AlexNet It has many of the features still used today in popular applications. Several techniques to achieve better results are widespread today : ReLU (Rectified Linear Unit) activation : \\(ReLU(x) = max(0,x)\\) Dropout of neurons : some neurons are arbritrarly deactivated ensembling several networks Data Augmentation : rotations, zooms of images Multiple GPUs computation of the backpropagation algorithm (Rumelhart, Hinton, and Williams (1986)) . For smaller projects, reusing already trained version of popular models on large datasets is also widespread, a technique named ‘pretraining’. It usually consists in keeping the convolution layers. 4.2 Model After several iterations, we have arrived at a model that uses some advanced techniques. We use Keras (Allaire and Chollet (2019)) to implement it. First, we load a pretrained version on ImageNet dataset of VGG16, a CNN developed at Oxford (Simonyan and Zisserman (2014)) . It uses ReLU activation, as seen in AlexNet. It also uses small, 3 by 3 convolution filters (also named receptive fields), which have been widely adopted in recent years. In comparison, AlexNet uses 11 by 11 receptive fields. The training is based on logarithmic loss: \\[H_{y&#39;} (y) := - \\sum_{i} y_{i}&#39; \\log (y_i)\\] \\(y_i\\) is the predicted probability value for class \\(i\\) and \\(y_i&#39;\\) is the true probability for that class. Then regularised by L2 weight decay : \\[\\mbox{Loss}(w,x) = \\mbox{RawLoss}(w,x) + \\frac{1}{2} \\,\\, c \\,\\|w\\|^2\\] On each optimization step in addition to stepping based on a gradient to better fit the training data, all model weights also decay proportionally towards zero by a small factor of \\((1-\\alpha c)\\) , with \\(\\alpha\\) denoting the learning rate (gradient step). The learning rate is also lowered as time passes, and filters are not initialized randomly. Dropout (ratio of \\(0.5\\)) is used in fully-connected layers. VGG16 structure is as follow. ## Model ## Model: &quot;vgg16&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## input_1 (InputLayer) [(None, 150, 150, 3)] 0 ## ________________________________________________________________________________ ## block1_conv1 (Conv2D) (None, 150, 150, 64) 1792 ## ________________________________________________________________________________ ## block1_conv2 (Conv2D) (None, 150, 150, 64) 36928 ## ________________________________________________________________________________ ## block1_pool (MaxPooling2D) (None, 75, 75, 64) 0 ## ________________________________________________________________________________ ## block2_conv1 (Conv2D) (None, 75, 75, 128) 73856 ## ________________________________________________________________________________ ## block2_conv2 (Conv2D) (None, 75, 75, 128) 147584 ## ________________________________________________________________________________ ## block2_pool (MaxPooling2D) (None, 37, 37, 128) 0 ## ________________________________________________________________________________ ## block3_conv1 (Conv2D) (None, 37, 37, 256) 295168 ## ________________________________________________________________________________ ## block3_conv2 (Conv2D) (None, 37, 37, 256) 590080 ## ________________________________________________________________________________ ## block3_conv3 (Conv2D) (None, 37, 37, 256) 590080 ## ________________________________________________________________________________ ## block3_pool (MaxPooling2D) (None, 18, 18, 256) 0 ## ________________________________________________________________________________ ## block4_conv1 (Conv2D) (None, 18, 18, 512) 1180160 ## ________________________________________________________________________________ ## block4_conv2 (Conv2D) (None, 18, 18, 512) 2359808 ## ________________________________________________________________________________ ## block4_conv3 (Conv2D) (None, 18, 18, 512) 2359808 ## ________________________________________________________________________________ ## block4_pool (MaxPooling2D) (None, 9, 9, 512) 0 ## ________________________________________________________________________________ ## block5_conv1 (Conv2D) (None, 9, 9, 512) 2359808 ## ________________________________________________________________________________ ## block5_conv2 (Conv2D) (None, 9, 9, 512) 2359808 ## ________________________________________________________________________________ ## block5_conv3 (Conv2D) (None, 9, 9, 512) 2359808 ## ________________________________________________________________________________ ## block5_pool (MaxPooling2D) (None, 4, 4, 512) 0 ## ================================================================================ ## Total params: 14,714,688 ## Trainable params: 14,714,688 ## Non-trainable params: 0 ## ________________________________________________________________________________ We keep the convolution/pooling layers, already trained, and replace the fully connected layer with our own. The last layer has a sigmoid activation to predict the constructor, either of Airbus or Boeing. ## Model ## Model: &quot;sequential&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## vgg16 (Model) (None, 4, 4, 512) 14714688 ## ________________________________________________________________________________ ## flatten (Flatten) (None, 8192) 0 ## ________________________________________________________________________________ ## dense (Dense) (None, 256) 2097408 ## ________________________________________________________________________________ ## dense_1 (Dense) (None, 1) 257 ## ================================================================================ ## Total params: 16,812,353 ## Trainable params: 16,812,353 ## Non-trainable params: 0 ## ________________________________________________________________________________ Then, we unfreeze previously frozen layers. This means that some convolution/pooling layers (those after “block3_conv1”) are going to be trained even though its original use was to bring its already trained architecture. This has shown to boost performance. ## Model ## Model: &quot;sequential&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## vgg16 (Model) (None, 4, 4, 512) 14714688 ## ________________________________________________________________________________ ## flatten (Flatten) (None, 8192) 0 ## ________________________________________________________________________________ ## dense (Dense) (None, 256) 2097408 ## ________________________________________________________________________________ ## dense_1 (Dense) (None, 1) 257 ## ================================================================================ ## Total params: 16,812,353 ## Trainable params: 16,552,193 ## Non-trainable params: 260,160 ## ________________________________________________________________________________ We perform some data augmentation: rotation, zoom, horizontal flip. train_datagen = image_data_generator( rescale = 1/255, rotation_range = 40, width_shift_range = 0.2, height_shift_range = 0.2, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = TRUE, fill_mode = &quot;nearest&quot; ) Skipping some technical details, we arrive at the training step. Here we use binary crossentropy (aka logarithmic loss) as a loss function. Moreover, we store classification accuracy, the ratio of number of correct predictions to the total number of images classified. \\[Acc = \\frac{TP + TN}{TP+TN+FP+FN}\\] where: TP = True positive; FP = False positive; TN = True negative; FN = False negative. model %&gt;% compile( loss = &quot;binary_crossentropy&quot;, optimizer = optimizer_rmsprop(lr = 1e-5), metrics = c(&quot;accuracy&quot;) ) Then we can train the model. Here we used 100 epochs with 100 steps by epoch. history &lt;- model %&gt;% fit_generator( train_generator, steps_per_epoch = 100, epochs = 100, validation_data = validation_generator, validation_steps = 50 ) 4.3 Performance check We iterated over several models. In the following images, we plot the logarithmic loss and the accuracy, on both the training and validation data for each of them. First, we implemented our own convolution layers and fully connected layers. In this model, we only use 30 epochs since we do not augment the dataset. Our loss reaches a minimum at around 15 epochs on the validation set. The validation accuracy begins to decrease after 25 epochs. The model achieves around 80% accuracy. Introductory model After this first iteration, we then performed image augmentation, which allow us to use 100 epochs. The validation loss reaches a minimum at around 80 epochs. However, accuracy on the validation set decreased to 70%. Model with augmented images At this point, we considered adding pretrained convolution layers from VGG16. We only train the fully connected layers, thus we use 30 epochs. Here, the validation and train loss are curiously intertwined. The accuracy does not reach a plateau, and is of 70% at the last epoch. Model with pretraining Lastly, we unfroze part of VGG16 convolution layers. This is the model discussed in the previous part. We resort to 100 epochs and observe a more classical loss, with the validation loss going up after 30 epochs while the training one continues downwards. The validation accuracy finally reaches 88%, plateauing at 30 epochs with 85% but still slowing growing afterwards. Final model References "],
["putting-into-production.html", "Part 5 Putting into production 5.1 REST API with Plumber 5.2 Docker", " Part 5 Putting into production 5.1 REST API with Plumber Technically, API stands for Application Program Interface — a name that’s both exactly what an API is and too vague to actually convey any meaning. But what are API’s really? They’re a simple way to set up a computer to pass information to other computers through the internet. 95% of the time, when people say API they mean a RESTful API, which is an API that uses HTTP to interact with other computers. HTTP is the same protocol your browser is using for you to read websites. When you type in a URL to a website, a computer receives your request and sends back HTML. RESTful API’s are the same thing, but instead of HTML they send back text or data. When you call an API you typically do one of two things. You either request that computer to send you a specific piece of information (like the weather of a certain city you requested), or you ask the other computer to change the data it has stored (like adding a record to a table). Most simply, API’s are ways to — from your computer — call a function on another computer. The easiest way to create an API in R is using the library plumber (Trestle Technology, LLC (2018)), a package which can convert existing R code into an API with just a few extra lines. First, we create a script that loads the data and trains the model. runtime_functions.R library(keras) model &lt;- load_model_hdf5(&quot;plane-spotter_pretrained_fine_tuning.h5&quot;) image_prediction &lt;- function(path){ image &lt;- image_load(path, target_size = c(150,150)) %&gt;% image_to_array() %&gt;% array_reshape(c(1, 150, 150, 3)) image_tensor &lt;- image/255 pred &lt;- predict_classes(model, image_tensor) if(pred == 0){ return(&quot;Airbus&quot;) } else{ return(&quot;Boeing&quot;) } } image_prediction(&quot;Plumber/Boeing.5023.jpg&quot;) The output is “Boeing” Then we built the GET endpoint : rest_controller.R source(&quot;runtime_functions.R&quot;) # Set an endpoint to return a pet name #* @get /names get_names &lt;- function(){ image_prediction(&quot;Plumber/Boeing.5023.jpg&quot;) } Here for the purpose of the exemple we predict a known image but the aim of the API is to get the image path as the input of the function. Finally, we need to use plumber to set up our R code to accept HTTP requests and transform them into executable R code. We simply: Import plumber. Show plumber where our endpoints are. Start the API service on port 80. Since HTTP defaults to port 80, our service to that port allows things we type in our browser to be executed in the R on our computer. Main.R library(plumber) r &lt;- plumb(&quot;rest_controller.R&quot;) r$run(port=80, host=&quot;0.0.0.0&quot;) Great, now we can see the result of the output either by typing curl -X GET “http://127.0.0.1:6974/names” -H “accept: application / json” on the terminal or directly by going to http://127.0.0.1:6974/names in your browser. 5.2 Docker As a data scientist, we sometimes want to have code running in places that are not our computer. In our case, we want to have our R script run an API continuously, regardless of if our laptop runs out of battery. One way we could set this up would be to create a virtual machine in a place like Amazon Web Services. A virtual machine is like a computer that you’ve used before, but it’s simulated–it doesn’t have it’s own process, memory, or hardware. Instead, a different computer is running the machine. Virtual machines are great because many of them can run on a single server, and they can be easily started and stopped. Docker is a way to make the process of configuring and running computers smoother. With Docker, you can create a single document that specifies how to set up the computer. The document lets you run these steps at a moments notice. More formally, docker is a way to run virtual machines as containers, which are lightweight executable packages of software. A container is an instance of an image, which is a snapshot of a computer at a moment in time. Images can be full snapshots, or they can just be a small addition to an earlier image. A dockerfile is the specification document for how to build the image. So we try to use the API we created as the code we want in our docker container. We’ll start with an image already setup to have Linux on it. Then we’ll create an image that has R installed on top of Linux. After that, we’ll add our R libraries into a third image. Lastly, we’ll transfer our files to the final image. A great thing about Docker is that since so many people use it, you can often find images that other people have made that do what you want to do. In our case, the Rocker project has a ton of Docker images that support R. In order to build an image, Docker always looks for a plain text file with no extension called dockerfile. To customize our docker container, we first specify our base container. For us, that’s the rocker/r-ver:3.5.0 image. We do this by using the FROM command. Then, we need to install the linux packages that are required for the plumber using linux bash commands. To these commands while setting up the image, we use the RUN. We then install python and the different usefull libraries, and COPY our code in the container. And that’s our dockerfile! It builds everything we need to run our API from any machine, regardless of the setup! All together it looks like this: FROM rocker/r-ver:3.6.0 # update some packages, including sodium and apache2, then clean RUN apt-get update \\ &amp;&amp; apt-get install -y --no-install-recommends \\ file \\ libcurl4-openssl-dev \\ libedit2 \\ libssl-dev \\ lsb-release \\ psmisc \\ procps \\ wget \\ libxml2-dev \\ libpq-dev \\ libssh2-1-dev \\ ca-certificates \\ libglib2.0-0 \\ libxext6 \\ libsm6 \\ libxrender1 \\ bzip2 \\ libsodium-dev \\ apache2 \\ zlib1g-dev \\ &amp;&amp; wget -O libssl1.0.0.deb http://ftp.debian.org/debian/pool/main/o/openssl/libssl1.0.0_1.0.1t-1+deb8u8_amd64.deb \\ &amp;&amp; dpkg -i libssl1.0.0.deb \\ &amp;&amp; rm libssl1.0.0.deb \\ &amp;&amp; apt-get clean \\ &amp;&amp; rm -rf /var/lib/apt/lists/ # install miniconda, and set the appropriate path variables. RUN wget --quiet https://repo.anaconda.com/miniconda/Miniconda3-4.6.14-Linux-x86_64.sh -O ~/miniconda.sh &amp;&amp; \\ /bin/bash ~/miniconda.sh -b -p /opt/conda &amp;&amp; \\ rm ~/miniconda.sh &amp;&amp; \\ /opt/conda/bin/conda clean -tipsy &amp;&amp; \\ ln -s /opt/conda/etc/profile.d/conda.sh /etc/profile.d/conda.sh &amp;&amp; \\ echo &quot;. /opt/conda/etc/profile.d/conda.sh&quot; &gt;&gt; ~/.bashrc &amp;&amp; \\ echo &quot;conda activate base&quot; &gt;&gt; ~/.bashrc ENV PATH /opt/conda/bin:$PATH # install tensorflow and h5py using the pip that links to miniconda (the default pip is for python 2.7) RUN /opt/conda/bin/pip install --no-cache-dir tensorflow==2.0.0 h5py==2.10.0 # install Pillow to load a unique image RUN /opt/conda/bin/pip install PIL RUN /opt/conda/bin/pip install Pillow # let R know the right version of python to use ENV RETICULATE_PYTHON /opt/conda/bin/python # copy the setup script, run it, then delete it COPY src/setup.R / RUN Rscript setup.R &amp;&amp; rm setup.R # copy all the other R files. COPY src /src EXPOSE 80 WORKDIR /src ENTRYPOINT [&quot;Rscript&quot;,&quot;main.R&quot;] To build our image, open a terminal in the project folder and run `docker build -t r-tensorflow-api . And in order to run the container : docker run --rm -it -p 80:80 r-tensorflow-api If you want to have this web service running on a server in the cloud, it only takes a few more steps. One way to do this is to use a tool like Amazon Elastic Container Service (ECS). ECS lets you take a container you’ve created and deploy it to AWS. At that point your container is running within AWS systems, and you will get an endpoint that anyone can hit with a browser from anywhere. References "],
["conclusion.html", "Part 6 Conclusion", " Part 6 Conclusion Our final model achieves around 88% accuracy, on validation images, at binary classification of Airbus versus Boeing airplanes. The next step would be to classify model types. To do so, we could chain another CNN: an image is classified under Airbus or Boeing by our binary CNN. the image is then fed into either one of two models trained to recognize model types of Boeing or Airbus, respectively. (e.g. if our first model outputs our image as Airbus, we feed the image in the Airbus model type CNN) The output is the model of the airplane. To achieve this, we need to scrape the website again. Sadly, the website planespotters.net was totally redesigned which made our scrapping code obsolete. In the end, we achieved a reasonable objective to scrap a website to obtain data, explore the data, train a deep learning model, create an API and a docker image. We also obtained a CNN able to differenciate Airbus and Boeing airplaines. Further advance would require rebuilding a scrapping tool. Moreover, our dataset size of around two thousand images (not counting augmentation), is a bit limited. In the computer vision community, several other methods are being developed which grow in popularity (Gu et al. (2015)). For example, stochastic gradient descent (or its relaxation to a few samples, minibatch) associated with batch normalization seem promising. Inception modules also allow to reduce the number of parameters (reduced to 5 millions which are much less than AlexNet with 60 millions). Residual Nets (ResNets) are also growing in popularity. References "],
["references.html", "References", " References "]
]
